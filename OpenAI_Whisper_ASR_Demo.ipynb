{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Web App Demonstrating OpenAI's Whisper Speech Recognition Model\n",
        "\n",
        "This is a Colab notebook that allows you to record or upload audio files to [OpenAI's free Whisper speech recognition model](https://openai.com/blog/whisper/). This was based on [an original notebook by @amrrs](https://github.com/amrrs/openai-whisper-webapp), with added documentation and test files by [Pete Warden](https://twitter.com/petewarden).\n",
        "\n",
        "To use it, choose `Runtime->Run All` from the Colab menu. If you're viewing this notebook on GitHub, follow [this link](https://colab.research.google.com/github/petewarden/openai-whisper-webapp/blob/main/OpenAI_Whisper_ASR_Demo.ipynb) to open it in Colab first. After about a minute or so, you should see a button at the bottom of the page with a `Record from microphone` link. Click this, you'll be asked to give permission to access your mic, and then speak for up to 30 seconds. Once you're done, press `Stop recording`, and a transcript of the first 30 seconds of your speech should soon appear in the box to the right of the recording button. To transcribe more speech, click `Clear' in the left box and start over.\n",
        "\n",
        "You can also upload your own audio samples using the folder icon on the left of this page. That gives you access to a file system you can upload to by dragging files into it. You can see examples of how to run the transcription in a couple of the cells below."
      ],
      "metadata": {
        "id": "Lbja1jB3vDOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Whisper Code"
      ],
      "metadata": {
        "id": "kosakhNmxb7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZsJUxc0aRsAf",
        "outputId": "31527395-caab-45e2-e1fc-f47410b2658c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the ML Model"
      ],
      "metadata": {
        "id": "AtAvuKSJxhNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n"
      ],
      "metadata": {
        "id": "Kr5faKybKi4p"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check we have a GPU\n",
        "\n",
        "You should see the output `device(type='cuda', index=0)` below. If you don't, you may be on a CPU-only Colab instance which will run more slowly. Go to `Runtime->Change Runtime Type` to fix this."
      ],
      "metadata": {
        "id": "e200RNNlxn5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_6_s2iHboR4",
        "outputId": "d4f0078e-e74d-41f7-9f79-aaff9a7267fd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Transcribe Function\n",
        "\n",
        "Now we've loaded the model, and have the code, this is the function that takes an audio file path as an input and returns the recognized text (and logs what it thinks the language is)."
      ],
      "metadata": {
        "id": "QwLTZtubySoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe(audio):\n",
        "\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "\n",
        "    # decode the audio\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    return result.text\n"
      ],
      "metadata": {
        "id": "JtTvvQQPcOZZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Web UI Toolkit\n",
        "\n",
        "We'll be using gradio to provide the widgets we need to do audio recording."
      ],
      "metadata": {
        "id": "9ojRF2zWzcYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio -q"
      ],
      "metadata": {
        "id": "fjM27tWsI4dH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time"
      ],
      "metadata": {
        "id": "ILFOYNnTcYe8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Upload Facility\n",
        "\n",
        "Upload your file in the prompt."
      ],
      "metadata": {
        "id": "2tHwfOG-zlY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "y2Zid2MKdPxK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "9f5cf8a4-1d45-46d1-8584-22e0c8123808"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c35224aa-8731-4d10-8931-983a394cfae0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c35224aa-8731-4d10-8931-983a394cfae0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving How DeepSeek Rewrote the Transformer [MLA].mp3 to How DeepSeek Rewrote the Transformer [MLA] (1).mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = next(iter(uploaded))\n",
        "print(filename)\n",
        "\n",
        "Audio(filename)\n",
        "\n",
        "hard_text = transcribe(filename)\n",
        "print(hard_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfP98XflppB5",
        "outputId": "4b5dd7ea-68c9-40d6-d09d-4b8065d25275"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How DeepSeek Rewrote the Transformer [MLA] (1).mp3\n",
            "Detected language: en\n",
            "This video is sponsored by KiwiCo, more on them later. In January 2025, the Chinese company DeepSeek shocked the world with the release of R1, a highly competitive language model that requires only a fraction of the compute of other leading models. Perhaps even more shocking is that unlike most of its American counterparts, DeepSeek has publicly released the R1 model weights, inference code, and extensive technical reports. Publishing an average of one report per month in 2024,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "from IPython.display import Audio, display\n",
        "import os\n",
        "\n",
        "# Load audio\n",
        "audio = AudioSegment.from_file(filename)\n",
        "\n",
        "# 5 seconds = 5000 ms\n",
        "frame_duration = 25 * 1000\n",
        "num_chunks = len(audio) // frame_duration + (1 if len(audio) % frame_duration > 0 else 0)\n",
        "\n",
        "# Optional: create a folder for chunks\n",
        "os.makedirs(\"chunks\", exist_ok=True)\n",
        "\n",
        "text = \"\"\n",
        "# Go through and play each chunk\n",
        "for i in range(num_chunks):\n",
        "    start = i * frame_duration\n",
        "    end = min((i + 1) * frame_duration, len(audio))\n",
        "    chunk = audio[start:end]\n",
        "\n",
        "    # Export to WAV file\n",
        "    chunk_filename = f\"chunks/chunk_{i+1}.wav\"\n",
        "    chunk.export(chunk_filename, format=\"wav\")\n",
        "\n",
        "    # print(f\"🔊 Playing chunk {i+1}: {start/1000:.2f}s to {end/1000:.2f}s\")\n",
        "    # display(Audio(chunk_filename))\n",
        "\n",
        "    text_chunks = transcribe(chunk_filename)\n",
        "    print(text_chunks)\n",
        "\n",
        "    text += text_chunks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J03L03DmrXoP",
        "outputId": "440f6a31-8c24-4141-8020-1971633c73c7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en\n",
            "This video is sponsored by KiwiCo, more on them later. In January 2025, the Chinese company DeepSeek shocked the world with the release of R1, a highly competitive language model that requires only a fraction of the compute of other leading models. Perhaps even more shocking is that unlike most of its American counterparts, DeepSeek has publicly released the R1 model weights, inference code, and extensive technical reports.\n",
            "Detected language: en\n",
            "reports, publishing an average of one report per month in 2024, and detailing many of the innovations that dramatically culminated in the release of R1 in early 2025. Back in June of 2024, the Deepseek team introduced a technique that they call multi-head latent attention. Unlike many deep-seek innovations that occur at the margins of the stack, multi-head latent attention strikes at the core of the transformer itself.\n",
            "Detected language: en\n",
            "This is the compute architecture that virtually all large language models share. This modification reduces the size of an important bottleneck, called the key value cache, by a factor of 57, allowing the model to generate text more than 6 times faster than a traditional transformer in deep-seek implementation. But how exactly was the deep-seek team able to squeeze such a significant improvement out of such a broadly used architecture?\n",
            "Detected language: en\n",
            "Like other modern language models, when you give deep-seek-a-prompt, the model generates its response one word fragment known as a token at a time. Mathematically, this auto-regressive approach means that each new token the model generates is a function of all the tokens that came before it. The interactions between tokens and large language models are handled by a mechanism called attention. Attention works by computing matrices called attention paths.\n",
            "Detected language: en\n",
            "These are the 144-tension patterns computed by the GPT-2 small model, when given the example input text the American flag is red, white, and. This model uses 12 separate attention mechanisms, called attention heads per layer, and has 12 layers, making for 144 total attention patterns. DeepSeq R1 has 128 attention heads per layer, and 61\n",
            "Detected language: en\n",
            "making for 7800 and 8 total patterns. In both models, the size of the attention pattern is equal to the number of tokens passed into the model. Our example input, the American flag is red, white, and maps to 9 tokens, so all of our attention patterns are 9 by 9 matrices.\n",
            "Detected language: en\n",
            "For example, this first attention pattern in the third layer of GPT-2 has a high value, mapping from the input token of American to the output token of flag. Meaning this attention head is likely applying the modifier American to the noun flag, creating a unified representation for the concept American flag. This eighth attention pattern in the eleventh layer has high value.\n",
            "Detected language: en\n",
            "values, mapping the words flag, red, and white to the output of the final token and. This attention head has pulled out words in our input that are relevant for predicting the correct next token of blue, which this GP22 small model does correctly predict. Let's take a bit deeper into exactly how the standard attention mechanism works in models like GP22 and build up a few equations, so we can make sense of how the deep-seek\n",
            "Detected language: en\n",
            "team made such a powerful improvement. To compute a given attention pattern, we take the input matrix X. This could be the input to any layer of our model and we'll have one row for each input token, and a number of columns that corresponds to the embedding dimension of the model. This is the length of the vector used to represent each token. GPT2 smalls embedding dimension is\n",
            "Detected language: en\n",
            "embedding dimension is 71.68. To compute a given attention pattern, we multiply our input matrix X by two separate sets of learned weights, WQ and WK. In GPT2, these matrices are of dimension 768 by 64, and result in two new matrices, Q and K, each of dimension at 9 by 64. The rows of our Q matrix are known\n",
            "Detected language: en\n",
            "known as queries, and the rows of archaic matrix are known as keys. The core idea of attention is that we now search for pairs of tokens that have similar queries and keys, allowing the model to learn various relationships between tokens. For example, a token like flag could query for words that modify its meaning, while words like American can produce keys in certain attention heads that flag them as modifiers.\n",
            "Detected language: en\n",
            "This modifier query and modifier key should produce similar key and query vectors. Mathematically, to find similar keys and queries, we can take the dot product of the keys and queries for each possible pair of our nine tokens. Similar keys and query vectors will generate high dot products. We can compute all these dot products at once by transposing our key matrix and multiplying by our query.\n",
            "Detected language: en\n",
            "matrix, resulting in a new 9 by 9 matrix, where each entry corresponds to the dot product of a single key and query. To compute our attention pattern, we apply a masking operation, effectively zeroing out the upper right portion of our matrix. This step is mostly important in training, as it prevents the model from cheating on its task of next token prediction by just looking at the next token.\n",
            "Detected language: en\n",
            "Finally, we normalize our result by dividing by the square root of our embedding dimension and applying a softmax operation, which forces each of the rows of our matrix to add to 1. Now that we've computed our attention pattern, we need to actually use it to process our data. This involves a couple more matrix multiplies. We first compute what's known as a value matrix by multiplying our input by a third weight matrix\n",
            "Detected language: en\n",
            "This computation is identical to the way we computed our keys and query matrices, just with a different set of learned weights. We then multiply our attention pattern matrix by our value matrix. This effectively takes a weighted sum of our values, following our attention pattern. One way to think about this step is as processing our inputs using a neural network, where the weights A are controlled by the\n",
            "Detected language: en\n",
            "data itself. Finally, the attention block in each layer has multiple heads. Each head performs the same computations but with different learned weights, resulting in a different set of queries, keys, attention patterns, and values for each head. The idea here is that various attention heads can specialize in various tasks, like searching for adjectives or searching for other instances of the same token. To compute the final output,\n",
            "Detected language: en\n",
            "output of the attention block. We stack the results from each head together and multiply by a final learned weight matrix, WO, giving us the final matrix output of our attention block. The attention block is a key part of modern language models, but requires a significant amount of computation. Since the height and width of our attention pattern are equal to the number of input tokens, the number of entries in this matrix scales as the number of\n",
            "Detected language: en\n",
            "input tokens is squared. This is potentially a huge computational problem for large models. OpenAI's chatGPT models now offer maximum context lengths of over 100,000 tokens. For reference, this is about the length of the first Harry Potter book. So computing each attention pattern for chatGPT's maximum allowed input size is equivalent to arranging the entire text of the book as a single row and column.\n",
            "Detected language: en\n",
            "and then computing dot products for every possible pair of tokens from the entire text. Fortunately, there's a huge computational shortcut that we can take. As large language models generate new text, a single token at a time, the attention patterns themselves don't actually change that much. In our American flag example, let's see the model generates a new token for the word blue.\n",
            "Detected language: en\n",
            "Our phrases now, the American flag is red, white, and blue. To see what the model says next, we now pass this new 10 token input back into the model to get the 11th token and so on. Our new 10 token input results in key query and value matrices each of dimension 10 by 64. But importantly, since our weight matrices apply the same identical operations to each token, the first nine rows of our key query and\n",
            "Detected language: en\n",
            "value matrices are unchanged from our original nine token input. Transposing our keys and multiplying by our queries to compute our new attention pattern, note that the first nine rows of Q and the first nine columns of K transpose are unchanged. This means that the upward left nine by nine matrix of our attention pattern will also be unchanged. And we only need to compute a new final row and column to arrive at our new 10 by 10.\n",
            "Detected language: en\n",
            "and attention pattern. And further, since we mask out the upper right corner of our attention pattern, we actually only need to compute the new bottom row. The bottom row of our attention pattern results from multiplying the final row of our query matrix by each column of our transposed key matrix. So to compute this final attention pattern row, we need to know all of our keys, but only the final new row of our query matrix.\n",
            "Detected language: en\n",
            "Since we already computed 9 out of 10 of our keys on the previous cloud of the model, it's much more computationally efficient to store these keys in memory and just access them when the new 10 token input comes along. The same logic applies to our value matrix. We need our full value matrix to compute our new outputs, but the first 9 rows are unchanged, so we can just cache them in memory. Note that there's no need to cache the queries.\n",
            "Detected language: en\n",
            "since we only need the new final rule of our queries to update our attention pattern. This idea is called key value or kV caching, and is a critical part of large language model infrastructure. Instead of compute growing quadratically as the square of the number of input tokens, key value caching means that the compute required by the model's attention blocks scales linearly with the number of input tokens. Now, this computational shortcut,\n",
            "Detected language: en\n",
            "does come at a cost, specifically increased memory usage. Our system must now store the keys and values for the full history of the model session for all attention heads across all layers in memory. Given a model with L layers, NH attention heads per layer, a dimension of DH for our key and value matrices and in input tokens, we must store 2 times N times DH times\n",
            "Detected language: en\n",
            "times NH times L unique numbers in our KV cache. Assuming floating point 16 numbers, the deep seek R1 architecture and a context length of 100,000 tokens, we end up needing to retrieve 4 megabytes per token in the model's context window, resulting in a huge 400 gigabytes of memory reads for each new token we compute. Deep seek solution to this problem is really clever.\n",
            "Detected language: en\n",
            "and it was great to be able to tinker with their inference code to really get my head around it. There's nothing quite like hands-on experimenting like this for developing understanding, which is why I was more than happy to partner again with this video sponsor KiwiCo. KiwiCo offers hands-on project kits that make learning genuinely fun for kids of all ages. My daughter is obsessed with colors and rainbows right now, and loves the color discovery crate.\n",
            "Detected language: en\n",
            "These spinners are such a fun way for us to explore color mixing together. It's amazing to see how the crates progress and build on each other. Last year she was developing fine motor skills as part of the Panda Club, and now in the Sprouts Club she's creating an experiment. When she turns six in a few years, she can join the KiwiCo Labs Club, where she'll get to work on more complex science and engineering projects, like this remote controlled car. I would have absolutely loved it.\n",
            "Detected language: en\n",
            "love this crate as a kid. My son is working on learning the names of colors. So far, everything is blue. This block puzzle is such a fun, interactive way from to explore different colors at his age. When my kids quickly get bored of or break many of their toys, we find ourselves continually coming back to their Kiwi co-creants. The build quality is really great, and the thoughtfulness and multipurpose design built into each crate really\n",
            "Detected language: en\n",
            "keeps them engaged. If you want your family to experience the awesomeness of KiwiCo, use my code WelchLabs to receive 50% off your first crate for kids 3 and older, or 20% off your first panda crate for kids under 3. Big thanks to KiwiCo for sponsoring this video. Now back to deep seek solution to the KVCache problem. Untenably large KVCaches are not a new problem. One popular solution is the KVCache.\n",
            "Detected language: en\n",
            "is to reuse key and value matrices across multiple attention heads. In multi-query attention blocks, instead of having unique key and value matrices for each attention head, we share a single key and value matrix across all heads. This reduces the required size of our KV cache by a significant factor of the number of heads per layer, 128 for the deep seek R1 architecture. However, this\n",
            "Detected language: en\n",
            "This modification does impact model performance, as forcing all attention heads to use the same keys and values allows for less specialization. A less destructor version of this idea is grouped query attention. Where instead of forcing all attention heads in a given layer to share the same key and value matrices, we create multiple groups of attention heads that share the same key and value matrices. Metaslamma 3 models use grouped query attention.\n",
            "Detected language: en\n",
            "with groups of 8 attention heads sharing the same key and value matrices, reducing the size of the KV cache by a factor of 8. Grouped query attention reduces KV cache size, but still takes a performance hit relative to full multi-head attention. Now what's really remarkable about deep-seek's approach, called a multi-head latent attention, is that they were able to reduce the needed KV cache size by a factor of 8.\n",
            "Detected language: en\n",
            "57 while actually improving performance. The key insight is a novel application of a very common idea in machine learning, a latent space. What if the model could learn to efficiently compress its own keys and values? Multi-head-laden attention effectively adds an extra step between each attention heads input and the key and value matrices. The idea is to project our\n",
            "Detected language: en\n",
            "input into a compressed latent space. That like multi-query attention is shared across all attention heads in a given block. However, unlike multi-query attention, where each head shares the same exact keys and values, in multi-head latent attention, the compressed latent space is projected back up to keys and value matrices using another set of learned weights, Wuk and WUV, where the weights are\n",
            "Detected language: en\n",
            "unique to each attention head. This gives multi-head latent attention more flexibility than multi-query attention or grouped query attention. Now at face value, since we've introduced a new matrix multiply, it appears that we've just traded some memory bandwidth for additional compute. And after all, the entire point of KV caching was to reduce the high compute needs of attention blocks. However, as the deep-seek team points out,\n",
            "Detected language: en\n",
            "With some clever linear algebra, we can rearrange our query computation to absorb the WUK weights and rearrange our final output computation to absorb the WUV weights. Since all these weights are fixed at training time, we only have to compute the absorbed weights once and can avoid any additional compute during inference. So when a new token comes along, we simultaneously compute its query\n",
            "Detected language: en\n",
            "vector and the queries projection into the latent cache space in one step. And then computer our attention pattern directly from the latent key value cache matrix. It's a really elegant solution. With multi-head latent attention, the size of the needed KV cache no longer has any dependence on the number of attention heads per layer, and instead just depends on the size of the shared KV cache matrix.\n",
            "Detected language: en\n",
            "For deep-seq R1, this is equal to the number of input tokens by 576. If implemented with traditional attention blocks, R1 would require four megabytes of KV-Cache per token. Grouped query attention with the group size of 8 would cut this down to 500 kilobytes per token, and multihide latent attention reduces the needed cache to only 70 kilobytes per token, a 57x reduction.\n",
            "Detected language: en\n",
            "What we're left with is a true improvement to the transformer architecture, enabling deep-seek R1 to generate tokens more than six times faster than of a Nella transformer, while actually improving algorithmic performance. Multi-head-related attention allows attention heads to share a key in value information in a more optimal way, where the model itself learns how to compress and share this information between attention heads.\n",
            "Detected language: en\n",
            "Our architecture is one of the most significant breakthroughs in modern AI history, and DeepSeq appears to have just made it work significantly better. It's amazing to see the path that DeepSeq carved through their 2024 papers, systematically making substantial improvements to models that required hundreds of millions of dollars in R&D and infrastructure costs. The stakes have never been higher for neural networks. It will be fascinating to see\n",
            "Detected language: en\n",
            "what new set of ideas unlocks the next level of capabilities, as we build more and more intelligent systems. If you enjoyed the graphics in this video, I think you'll really like the poster version. The poster includes a walkthrough of multi-head latent attention with detailed captions, and I've rearranged the flow a bit to work better as a poster. On the bottom, I've included a detailed comparison between various forms of attention, including the required\n",
            "Detected language: en\n",
            "sizes of the KV caches and a 3D model of each attention block. The matrix images in the video and poster are actually from the real deep seek model. I'm mostly showing the weights from the first layer of deep seek V3. The poster looks great in a simple frame that you can pick up on Amazon, and is a great way to see how MLA works and just a nice way to decorate your walls. I'm offering free shipping on the poster for a limited time at WelchLabs.com.\n",
            "Detected language: en\n",
            "or you can pick it up as a limited edition bundle with a signed copy of my imaginary numbers book. Finally, big thank you to everyone who's purchased from the Welch Lab store. Your purchases go a long way to helping me make more great videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIUNB7a7ty_P",
        "outputId": "ca49d648-e116-472b-ab92-5b67591c8138"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This video is sponsored by KiwiCo, more on them later. In January 2025, the Chinese company DeepSeek shocked the world with the release of R1, a highly competitive language model that requires only a fraction of the compute of other leading models. Perhaps even more shocking is that unlike most of its American counterparts, DeepSeek has publicly released the R1 model weights, inference code, and extensive technical reports.reports, publishing an average of one report per month in 2024, and detailing many of the innovations that dramatically culminated in the release of R1 in early 2025. Back in June of 2024, the Deepseek team introduced a technique that they call multi-head latent attention. Unlike many deep-seek innovations that occur at the margins of the stack, multi-head latent attention strikes at the core of the transformer itself.This is the compute architecture that virtually all large language models share. This modification reduces the size of an important bottleneck, called the key value cache, by a factor of 57, allowing the model to generate text more than 6 times faster than a traditional transformer in deep-seek implementation. But how exactly was the deep-seek team able to squeeze such a significant improvement out of such a broadly used architecture?Like other modern language models, when you give deep-seek-a-prompt, the model generates its response one word fragment known as a token at a time. Mathematically, this auto-regressive approach means that each new token the model generates is a function of all the tokens that came before it. The interactions between tokens and large language models are handled by a mechanism called attention. Attention works by computing matrices called attention paths.These are the 144-tension patterns computed by the GPT-2 small model, when given the example input text the American flag is red, white, and. This model uses 12 separate attention mechanisms, called attention heads per layer, and has 12 layers, making for 144 total attention patterns. DeepSeq R1 has 128 attention heads per layer, and 61making for 7800 and 8 total patterns. In both models, the size of the attention pattern is equal to the number of tokens passed into the model. Our example input, the American flag is red, white, and maps to 9 tokens, so all of our attention patterns are 9 by 9 matrices.For example, this first attention pattern in the third layer of GPT-2 has a high value, mapping from the input token of American to the output token of flag. Meaning this attention head is likely applying the modifier American to the noun flag, creating a unified representation for the concept American flag. This eighth attention pattern in the eleventh layer has high value.values, mapping the words flag, red, and white to the output of the final token and. This attention head has pulled out words in our input that are relevant for predicting the correct next token of blue, which this GP22 small model does correctly predict. Let's take a bit deeper into exactly how the standard attention mechanism works in models like GP22 and build up a few equations, so we can make sense of how the deep-seekteam made such a powerful improvement. To compute a given attention pattern, we take the input matrix X. This could be the input to any layer of our model and we'll have one row for each input token, and a number of columns that corresponds to the embedding dimension of the model. This is the length of the vector used to represent each token. GPT2 smalls embedding dimension isembedding dimension is 71.68. To compute a given attention pattern, we multiply our input matrix X by two separate sets of learned weights, WQ and WK. In GPT2, these matrices are of dimension 768 by 64, and result in two new matrices, Q and K, each of dimension at 9 by 64. The rows of our Q matrix are knownknown as queries, and the rows of archaic matrix are known as keys. The core idea of attention is that we now search for pairs of tokens that have similar queries and keys, allowing the model to learn various relationships between tokens. For example, a token like flag could query for words that modify its meaning, while words like American can produce keys in certain attention heads that flag them as modifiers.This modifier query and modifier key should produce similar key and query vectors. Mathematically, to find similar keys and queries, we can take the dot product of the keys and queries for each possible pair of our nine tokens. Similar keys and query vectors will generate high dot products. We can compute all these dot products at once by transposing our key matrix and multiplying by our query.matrix, resulting in a new 9 by 9 matrix, where each entry corresponds to the dot product of a single key and query. To compute our attention pattern, we apply a masking operation, effectively zeroing out the upper right portion of our matrix. This step is mostly important in training, as it prevents the model from cheating on its task of next token prediction by just looking at the next token.Finally, we normalize our result by dividing by the square root of our embedding dimension and applying a softmax operation, which forces each of the rows of our matrix to add to 1. Now that we've computed our attention pattern, we need to actually use it to process our data. This involves a couple more matrix multiplies. We first compute what's known as a value matrix by multiplying our input by a third weight matrixThis computation is identical to the way we computed our keys and query matrices, just with a different set of learned weights. We then multiply our attention pattern matrix by our value matrix. This effectively takes a weighted sum of our values, following our attention pattern. One way to think about this step is as processing our inputs using a neural network, where the weights A are controlled by thedata itself. Finally, the attention block in each layer has multiple heads. Each head performs the same computations but with different learned weights, resulting in a different set of queries, keys, attention patterns, and values for each head. The idea here is that various attention heads can specialize in various tasks, like searching for adjectives or searching for other instances of the same token. To compute the final output,output of the attention block. We stack the results from each head together and multiply by a final learned weight matrix, WO, giving us the final matrix output of our attention block. The attention block is a key part of modern language models, but requires a significant amount of computation. Since the height and width of our attention pattern are equal to the number of input tokens, the number of entries in this matrix scales as the number ofinput tokens is squared. This is potentially a huge computational problem for large models. OpenAI's chatGPT models now offer maximum context lengths of over 100,000 tokens. For reference, this is about the length of the first Harry Potter book. So computing each attention pattern for chatGPT's maximum allowed input size is equivalent to arranging the entire text of the book as a single row and column.and then computing dot products for every possible pair of tokens from the entire text. Fortunately, there's a huge computational shortcut that we can take. As large language models generate new text, a single token at a time, the attention patterns themselves don't actually change that much. In our American flag example, let's see the model generates a new token for the word blue.Our phrases now, the American flag is red, white, and blue. To see what the model says next, we now pass this new 10 token input back into the model to get the 11th token and so on. Our new 10 token input results in key query and value matrices each of dimension 10 by 64. But importantly, since our weight matrices apply the same identical operations to each token, the first nine rows of our key query andvalue matrices are unchanged from our original nine token input. Transposing our keys and multiplying by our queries to compute our new attention pattern, note that the first nine rows of Q and the first nine columns of K transpose are unchanged. This means that the upward left nine by nine matrix of our attention pattern will also be unchanged. And we only need to compute a new final row and column to arrive at our new 10 by 10.and attention pattern. And further, since we mask out the upper right corner of our attention pattern, we actually only need to compute the new bottom row. The bottom row of our attention pattern results from multiplying the final row of our query matrix by each column of our transposed key matrix. So to compute this final attention pattern row, we need to know all of our keys, but only the final new row of our query matrix.Since we already computed 9 out of 10 of our keys on the previous cloud of the model, it's much more computationally efficient to store these keys in memory and just access them when the new 10 token input comes along. The same logic applies to our value matrix. We need our full value matrix to compute our new outputs, but the first 9 rows are unchanged, so we can just cache them in memory. Note that there's no need to cache the queries.since we only need the new final rule of our queries to update our attention pattern. This idea is called key value or kV caching, and is a critical part of large language model infrastructure. Instead of compute growing quadratically as the square of the number of input tokens, key value caching means that the compute required by the model's attention blocks scales linearly with the number of input tokens. Now, this computational shortcut,does come at a cost, specifically increased memory usage. Our system must now store the keys and values for the full history of the model session for all attention heads across all layers in memory. Given a model with L layers, NH attention heads per layer, a dimension of DH for our key and value matrices and in input tokens, we must store 2 times N times DH timestimes NH times L unique numbers in our KV cache. Assuming floating point 16 numbers, the deep seek R1 architecture and a context length of 100,000 tokens, we end up needing to retrieve 4 megabytes per token in the model's context window, resulting in a huge 400 gigabytes of memory reads for each new token we compute. Deep seek solution to this problem is really clever.and it was great to be able to tinker with their inference code to really get my head around it. There's nothing quite like hands-on experimenting like this for developing understanding, which is why I was more than happy to partner again with this video sponsor KiwiCo. KiwiCo offers hands-on project kits that make learning genuinely fun for kids of all ages. My daughter is obsessed with colors and rainbows right now, and loves the color discovery crate.These spinners are such a fun way for us to explore color mixing together. It's amazing to see how the crates progress and build on each other. Last year she was developing fine motor skills as part of the Panda Club, and now in the Sprouts Club she's creating an experiment. When she turns six in a few years, she can join the KiwiCo Labs Club, where she'll get to work on more complex science and engineering projects, like this remote controlled car. I would have absolutely loved it.love this crate as a kid. My son is working on learning the names of colors. So far, everything is blue. This block puzzle is such a fun, interactive way from to explore different colors at his age. When my kids quickly get bored of or break many of their toys, we find ourselves continually coming back to their Kiwi co-creants. The build quality is really great, and the thoughtfulness and multipurpose design built into each crate reallykeeps them engaged. If you want your family to experience the awesomeness of KiwiCo, use my code WelchLabs to receive 50% off your first crate for kids 3 and older, or 20% off your first panda crate for kids under 3. Big thanks to KiwiCo for sponsoring this video. Now back to deep seek solution to the KVCache problem. Untenably large KVCaches are not a new problem. One popular solution is the KVCache.is to reuse key and value matrices across multiple attention heads. In multi-query attention blocks, instead of having unique key and value matrices for each attention head, we share a single key and value matrix across all heads. This reduces the required size of our KV cache by a significant factor of the number of heads per layer, 128 for the deep seek R1 architecture. However, thisThis modification does impact model performance, as forcing all attention heads to use the same keys and values allows for less specialization. A less destructor version of this idea is grouped query attention. Where instead of forcing all attention heads in a given layer to share the same key and value matrices, we create multiple groups of attention heads that share the same key and value matrices. Metaslamma 3 models use grouped query attention.with groups of 8 attention heads sharing the same key and value matrices, reducing the size of the KV cache by a factor of 8. Grouped query attention reduces KV cache size, but still takes a performance hit relative to full multi-head attention. Now what's really remarkable about deep-seek's approach, called a multi-head latent attention, is that they were able to reduce the needed KV cache size by a factor of 8.57 while actually improving performance. The key insight is a novel application of a very common idea in machine learning, a latent space. What if the model could learn to efficiently compress its own keys and values? Multi-head-laden attention effectively adds an extra step between each attention heads input and the key and value matrices. The idea is to project ourinput into a compressed latent space. That like multi-query attention is shared across all attention heads in a given block. However, unlike multi-query attention, where each head shares the same exact keys and values, in multi-head latent attention, the compressed latent space is projected back up to keys and value matrices using another set of learned weights, Wuk and WUV, where the weights areunique to each attention head. This gives multi-head latent attention more flexibility than multi-query attention or grouped query attention. Now at face value, since we've introduced a new matrix multiply, it appears that we've just traded some memory bandwidth for additional compute. And after all, the entire point of KV caching was to reduce the high compute needs of attention blocks. However, as the deep-seek team points out,With some clever linear algebra, we can rearrange our query computation to absorb the WUK weights and rearrange our final output computation to absorb the WUV weights. Since all these weights are fixed at training time, we only have to compute the absorbed weights once and can avoid any additional compute during inference. So when a new token comes along, we simultaneously compute its queryvector and the queries projection into the latent cache space in one step. And then computer our attention pattern directly from the latent key value cache matrix. It's a really elegant solution. With multi-head latent attention, the size of the needed KV cache no longer has any dependence on the number of attention heads per layer, and instead just depends on the size of the shared KV cache matrix.For deep-seq R1, this is equal to the number of input tokens by 576. If implemented with traditional attention blocks, R1 would require four megabytes of KV-Cache per token. Grouped query attention with the group size of 8 would cut this down to 500 kilobytes per token, and multihide latent attention reduces the needed cache to only 70 kilobytes per token, a 57x reduction.What we're left with is a true improvement to the transformer architecture, enabling deep-seek R1 to generate tokens more than six times faster than of a Nella transformer, while actually improving algorithmic performance. Multi-head-related attention allows attention heads to share a key in value information in a more optimal way, where the model itself learns how to compress and share this information between attention heads.Our architecture is one of the most significant breakthroughs in modern AI history, and DeepSeq appears to have just made it work significantly better. It's amazing to see the path that DeepSeq carved through their 2024 papers, systematically making substantial improvements to models that required hundreds of millions of dollars in R&D and infrastructure costs. The stakes have never been higher for neural networks. It will be fascinating to seewhat new set of ideas unlocks the next level of capabilities, as we build more and more intelligent systems. If you enjoyed the graphics in this video, I think you'll really like the poster version. The poster includes a walkthrough of multi-head latent attention with detailed captions, and I've rearranged the flow a bit to work better as a poster. On the bottom, I've included a detailed comparison between various forms of attention, including the requiredsizes of the KV caches and a 3D model of each attention block. The matrix images in the video and poster are actually from the real deep seek model. I'm mostly showing the weights from the first layer of deep seek V3. The poster looks great in a simple frame that you can pick up on Amazon, and is a great way to see how MLA works and just a nice way to decorate your walls. I'm offering free shipping on the poster for a limited time at WelchLabs.com.or you can pick it up as a limited edition bundle with a signed copy of my imaginary numbers book. Finally, big thank you to everyone who's purchased from the Welch Lab store. Your purchases go a long way to helping me make more great videos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "file_path = f\"/content/drive/My Drive/{filename.split('.mp3')}.txt\"\n",
        "print(file_path)\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DWlQGVtxIHO",
        "outputId": "54b7611f-636a-4ac4-bc04-c05b02437867"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/['How DeepSeek Rewrote the Transformer [MLA] (1)', ''].txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdYnjFAVx8Nm"
      },
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}